{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9419680,"sourceType":"datasetVersion","datasetId":5721195}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport pandas as pd\nfrom sklearn.utils import resample\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizer\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support, roc_auc_score\nimport numpy as np\n\n# Function to load and process the HDF5 file\ndef load_and_process_hdf5(file_path):\n    with h5py.File(file_path, 'r') as hdf:\n        cwe_119_data = pd.Series(hdf['CWE-119'][:], name='CWE-119')\n        cwe_120_data = pd.Series(hdf['CWE-120'][:], name='CWE-120')\n        cwe_469_data = pd.Series(hdf['CWE-469'][:], name='CWE-469')\n        cwe_476_data = pd.Series(hdf['CWE-476'][:], name='CWE-476')\n        cwe_other_data = pd.Series(hdf['CWE-other'][:], name='CWE-other')\n        function_source_data = pd.Series(hdf['functionSource'][:], name='functionSource')\n\n    df = pd.concat([cwe_119_data, cwe_120_data, cwe_469_data, cwe_476_data, cwe_other_data], axis=1)\n\n    def assign_class(row):\n        if row['CWE-119']:\n            return 0\n        elif row['CWE-120']:\n            return 1\n        elif row['CWE-469']:\n            return 2\n        elif row['CWE-476']:\n            return 3\n        elif row['CWE-other']:\n            return 4\n        else:\n            return -1\n\n    df['Class'] = df.apply(assign_class, axis=1)\n    mask = df['Class'] != -1\n    df_filtered = df[mask]\n    function_source_filtered = function_source_data[mask]\n\n    df_final = pd.concat([df_filtered['Class'], function_source_filtered], axis=1)\n    return df_final\n\n# Paths to HDF5 files\ntrain_hdf5_file_path = '/kaggle/input/vulnercode/VDISC_train.hdf5'\ntest_hdf5_file_path = '/kaggle/input/vulnercode/VDISC_test.hdf5'\nvalidation_hdf5_file_path = '/kaggle/input/vulnercode/VDISC_validate.hdf5'\n\n# Process the datasets\ndf_train_final = load_and_process_hdf5(train_hdf5_file_path)\ndf_val_final = load_and_process_hdf5(validation_hdf5_file_path)\ndf_test_final = load_and_process_hdf5(test_hdf5_file_path)\n\n# Downsample datasets\ntrain_sample_proportions = {0: 5942, 1: 5777, 4: 5582, 3: 2755, 2: 249}\ndf_train_downsampled = pd.DataFrame()\nfor cls, n_samples in train_sample_proportions.items():\n    class_data = df_train_final[df_train_final['Class'] == cls]\n    class_downsampled = resample(class_data, replace=False, n_samples=n_samples, random_state=42)\n    df_train_downsampled = pd.concat([df_train_downsampled, class_downsampled])\n\nval_sample_proportions = {0: 1142, 1: 1099, 4: 1071, 3: 535, 2: 53}\ndf_val_downsampled = pd.DataFrame()\nfor cls, n_samples in val_sample_proportions.items():\n    class_data = df_val_final[df_val_final['Class'] == cls]\n    class_downsampled = resample(class_data, replace=False, n_samples=n_samples, random_state=42)\n    df_val_downsampled = pd.concat([df_val_downsampled, class_downsampled])\n\ndf_test_downsampled = pd.DataFrame()\nfor cls, n_samples in val_sample_proportions.items():\n    class_data = df_test_final[df_test_final['Class'] == cls]\n    class_downsampled = resample(class_data, replace=False, n_samples=n_samples, random_state=42)\n    df_test_downsampled = pd.concat([df_test_downsampled, class_downsampled])\n\n# Custom Dataset class to handle encodings and labels\nclass CodeBERTDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {'input_ids': self.encodings[idx]}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Function to tokenize data for LSTM\ndef tokenize_function_lstm(df, tokenizer):\n    return tokenizer(\n        df['functionSource'].astype(str).tolist(),\n        padding=True,\n        truncation=True,\n        max_length=512,\n        return_tensors='pt'\n    )['input_ids']\n\n# Tokenize the data using RobertaTokenizer\ngraphcodebert_tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\ntrain_encodings_lstm = tokenize_function_lstm(df_train_downsampled, graphcodebert_tokenizer)\nval_encodings_lstm = tokenize_function_lstm(df_val_downsampled, graphcodebert_tokenizer)\ntest_encodings_lstm = tokenize_function_lstm(df_test_downsampled, graphcodebert_tokenizer)\n\ntrain_labels = df_train_downsampled['Class'].tolist()\nval_labels = df_val_downsampled['Class'].tolist()\ntest_labels = df_test_downsampled['Class'].tolist()\n\n# LSTM Model with Attention\nclass AttentionLSTM(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, attention_dim):\n        super(AttentionLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.attention_layer = nn.Linear(hidden_dim, attention_dim)\n        self.context_vector = nn.Linear(attention_dim, 1, bias=False)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def attention(self, lstm_output):\n        attention_scores = torch.tanh(self.attention_layer(lstm_output))\n        attention_weights = torch.softmax(self.context_vector(attention_scores), dim=1)\n        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n        return context_vector\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_output, _ = self.lstm(embedded)\n        attention_output = self.attention(lstm_output)\n        output = self.fc(attention_output)\n        return output\n\n# Ensure vocab_size matches the tokenizer's vocab size\nvocab_size = graphcodebert_tokenizer.vocab_size  \nembedding_dim = 128\nhidden_dim = 256\nattention_dim = 64\noutput_dim = 5  # 5 classes\n\n# Instantiate the model, loss function, and optimizer\nmodel = AttentionLSTM(embedding_dim, hidden_dim, output_dim, vocab_size, attention_dim)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Create Dataloaders\ntrain_dataset = CodeBERTDataset(train_encodings_lstm, train_labels)\nval_dataset = CodeBERTDataset(val_encodings_lstm, val_labels)\ntest_dataset = CodeBERTDataset(test_encodings_lstm, test_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Training function with AUC, Precision, Recall, F1 score, and Classification Report\ndef train_model(model, train_loader, val_loader, epochs=10):\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            inputs = batch['input_ids']\n            labels = batch['labels']\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss, correct, total = 0, 0, 0\n        all_labels = []\n        all_preds = []\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = batch['input_ids']\n                labels = batch['labels']\n                outputs = model(inputs)\n                val_loss += criterion(outputs, labels).item()\n                _, predicted = torch.max(outputs, 1)\n                correct += (predicted == labels).sum().item()\n                total += labels.size(0)\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(predicted.cpu().numpy())\n\n        accuracy = correct / total\n        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n        auc_score = roc_auc_score(np.eye(output_dim)[all_labels], np.eye(output_dim)[all_preds], multi_class=\"ovr\")\n\n        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}, Accuracy: {accuracy:.4f}, \"\n              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, AUC: {auc_score:.4f}\")\n\n        # Classification Report\n        print(\"\\nClassification Report (Validation):\")\n        print(classification_report(all_labels, all_preds, digits=4))\n\n# Test the model with AUC, Precision, Recall, F1 score, and Classification Report\ndef test_model(model, test_loader):\n    model.eval()\n    correct, total = 0, 0\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = batch['input_ids']\n            labels = batch['labels']\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n    accuracy = correct / total\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n    auc_score = roc_auc_score(np.eye(output_dim)[all_labels], np.eye(output_dim)[all_preds], multi_class=\"ovr\")\n\n    print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, \"\n          f\"F1 Score: {f1:.4f}, AUC: {auc_score:.4f}\")\n\n    # Classification Report\n    print(\"\\nClassification Report (Test):\")\n    print(classification_report(all_labels, all_preds, digits=4))\n\n# Train the model for 10 epochs\ntrain_model(model, train_loader, val_loader, epochs=10)\n\n# Evaluate the model on the test set\ntest_model(model, test_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T06:53:04.898269Z","iopub.execute_input":"2024-09-18T06:53:04.898663Z","iopub.status.idle":"2024-09-18T07:52:08.940401Z","shell.execute_reply.started":"2024-09-18T06:53:04.898625Z","shell.execute_reply":"2024-09-18T07:52:08.939420Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88cb00872d6a4c39ad0e2886f5baf830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e953bf00143f45af92ddea593b3af028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ab87929815e4f6eab8f8d0fa179b857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5de3834fbd645118e696c1e374d1b95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/539 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d14d66966a044162ab6d005db2305733"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.9297\nValidation Loss: 0.7487, Accuracy: 0.7272, Precision: 0.7442, Recall: 0.7272, F1 Score: 0.7249, AUC: 0.7481\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8790    0.8590    0.8689      1142\n           1     0.7980    0.7298    0.7624      1099\n           2     1.0000    0.0189    0.0370        53\n           3     0.6780    0.5234    0.5907       535\n           4     0.5656    0.7208    0.6338      1071\n\n    accuracy                         0.7272      3900\n   macro avg     0.7841    0.5704    0.5786      3900\nweighted avg     0.7442    0.7272    0.7249      3900\n\nEpoch 2, Loss: 0.6845\nValidation Loss: 0.6899, Accuracy: 0.7549, Precision: 0.7629, Recall: 0.7549, F1 Score: 0.7551, AUC: 0.7754\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8964    0.8485    0.8718      1142\n           1     0.8371    0.7716    0.8030      1099\n           2     0.5000    0.0943    0.1587        53\n           3     0.6453    0.6393    0.6423       535\n           4     0.6161    0.7283    0.6675      1071\n\n    accuracy                         0.7549      3900\n   macro avg     0.6990    0.6164    0.6287      3900\nweighted avg     0.7629    0.7549    0.7551      3900\n\nEpoch 3, Loss: 0.5955\nValidation Loss: 0.6783, Accuracy: 0.7600, Precision: 0.7634, Recall: 0.7600, F1 Score: 0.7572, AUC: 0.7743\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8961    0.8459    0.8703      1142\n           1     0.7807    0.8262    0.8028      1099\n           2     0.6667    0.0755    0.1356        53\n           3     0.7032    0.6112    0.6540       535\n           4     0.6389    0.7087    0.6720      1071\n\n    accuracy                         0.7600      3900\n   macro avg     0.7371    0.6135    0.6269      3900\nweighted avg     0.7634    0.7600    0.7572      3900\n\nEpoch 4, Loss: 0.4990\nValidation Loss: 0.7154, Accuracy: 0.7556, Precision: 0.7622, Recall: 0.7556, F1 Score: 0.7551, AUC: 0.7798\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8888    0.8468    0.8673      1142\n           1     0.8093    0.7880    0.7985      1099\n           2     0.5625    0.1698    0.2609        53\n           3     0.7034    0.5850    0.6388       535\n           4     0.6183    0.7395    0.6735      1071\n\n    accuracy                         0.7556      3900\n   macro avg     0.7165    0.6258    0.6478      3900\nweighted avg     0.7622    0.7556    0.7551      3900\n\nEpoch 5, Loss: 0.3964\nValidation Loss: 0.7659, Accuracy: 0.7562, Precision: 0.7555, Recall: 0.7562, F1 Score: 0.7545, AUC: 0.7862\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8678    0.8450    0.8563      1142\n           1     0.7774    0.8071    0.7920      1099\n           2     0.5238    0.2075    0.2973        53\n           3     0.6780    0.6336    0.6551       535\n           4     0.6634    0.6975    0.6800      1071\n\n    accuracy                         0.7562      3900\n   macro avg     0.7021    0.6382    0.6561      3900\nweighted avg     0.7555    0.7562    0.7545      3900\n\nEpoch 6, Loss: 0.2957\nValidation Loss: 0.8596, Accuracy: 0.7541, Precision: 0.7514, Recall: 0.7541, F1 Score: 0.7494, AUC: 0.7798\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8427    0.8634    0.8529      1142\n           1     0.7496    0.8335    0.7893      1099\n           2     0.5556    0.1887    0.2817        53\n           3     0.7373    0.5720    0.6442       535\n           4     0.6726    0.6751    0.6738      1071\n\n    accuracy                         0.7541      3900\n   macro avg     0.7116    0.6265    0.6484      3900\nweighted avg     0.7514    0.7541    0.7494      3900\n\nEpoch 7, Loss: 0.2156\nValidation Loss: 1.0029, Accuracy: 0.7423, Precision: 0.7378, Recall: 0.7423, F1 Score: 0.7391, AUC: 0.7861\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8099    0.8616    0.8350      1142\n           1     0.7661    0.8016    0.7835      1099\n           2     0.3684    0.2642    0.3077        53\n           3     0.6592    0.6617    0.6604       535\n           4     0.6896    0.6181    0.6519      1071\n\n    accuracy                         0.7423      3900\n   macro avg     0.6586    0.6414    0.6477      3900\nweighted avg     0.7378    0.7423    0.7391      3900\n\nEpoch 8, Loss: 0.1505\nValidation Loss: 1.0849, Accuracy: 0.7408, Precision: 0.7384, Recall: 0.7408, F1 Score: 0.7392, AUC: 0.7826\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8278    0.8249    0.8263      1142\n           1     0.7670    0.7998    0.7831      1099\n           2     0.3824    0.2453    0.2989        53\n           3     0.6746    0.6393    0.6564       535\n           4     0.6633    0.6657    0.6645      1071\n\n    accuracy                         0.7408      3900\n   macro avg     0.6630    0.6350    0.6458      3900\nweighted avg     0.7384    0.7408    0.7392      3900\n\nEpoch 9, Loss: 0.1101\nValidation Loss: 1.2925, Accuracy: 0.7354, Precision: 0.7338, Recall: 0.7354, F1 Score: 0.7335, AUC: 0.7792\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8223    0.8389    0.8305      1142\n           1     0.7496    0.7789    0.7639      1099\n           2     0.3684    0.2642    0.3077        53\n           3     0.7178    0.5944    0.6503       535\n           4     0.6493    0.6741    0.6615      1071\n\n    accuracy                         0.7354      3900\n   macro avg     0.6615    0.6301    0.6428      3900\nweighted avg     0.7338    0.7354    0.7335      3900\n\nEpoch 10, Loss: 0.0786\nValidation Loss: 1.3237, Accuracy: 0.7390, Precision: 0.7373, Recall: 0.7390, F1 Score: 0.7379, AUC: 0.7874\n\nClassification Report (Validation):\n              precision    recall  f1-score   support\n\n           0     0.8197    0.8363    0.8279      1142\n           1     0.7713    0.7916    0.7813      1099\n           2     0.3200    0.3019    0.3107        53\n           3     0.6990    0.6467    0.6718       535\n           4     0.6544    0.6489    0.6517      1071\n\n    accuracy                         0.7390      3900\n   macro avg     0.6529    0.6451    0.6487      3900\nweighted avg     0.7373    0.7390    0.7379      3900\n\nTest Accuracy: 0.7300, Precision: 0.7297, Recall: 0.7300, F1 Score: 0.7295, AUC: 0.7743\n\nClassification Report (Test):\n              precision    recall  f1-score   support\n\n           0     0.8374    0.8161    0.8266      1142\n           1     0.7604    0.7971    0.7783      1099\n           2     0.2500    0.2264    0.2376        53\n           3     0.6721    0.6131    0.6413       535\n           4     0.6360    0.6527    0.6442      1071\n\n    accuracy                         0.7300      3900\n   macro avg     0.6312    0.6211    0.6256      3900\nweighted avg     0.7297    0.7300    0.7295      3900\n\n","output_type":"stream"}]}]}